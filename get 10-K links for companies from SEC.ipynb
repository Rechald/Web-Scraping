{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Part A - Getting the data on 10 K filings in a dataframe (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this part of the assignment you are provided a list of mutual funds and the companies that were dropped from their portfolios.  We want to obtain the 10-K filings for these companies.  In the file (Dropped_Companies.csv), there is a column called \"Name of Stock\".  Load the file, extract a list of all the companies in that column, then find the 10-K links for all these companies.  Feel free to utilize Beautiful Soup and Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, start with the importing of the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will be populating the following lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weblink = []\n",
    "filing_type = []\n",
    "company_name = []\n",
    "date = []\n",
    "url=[]\n",
    "finalurl=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the Dropped_Companies.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dropped_Companies.csv')\n",
    "name_of_stock=df['Name of Stock']\n",
    "list_name=list(name_of_stock)\n",
    "for i in range(0,len(list_name)):\n",
    "    list_name[i] = list_name[i].replace(' ','+')\n",
    "    url.append('https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&company=' +list_name[i] + '&type=10-K&dateb=&owner=exclude&count=100')\n",
    "#print(len(url))\n",
    "\n",
    "\n",
    "for i in range(0, len(url)):\n",
    "    html = urllib.request.urlopen(url[i]).read().decode('utf-8')\n",
    "    if html.find('Filing Date') != -1:\n",
    "        finalurl.append(url[i])\n",
    "#print(len(finalurl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The information on 10-K, 10-K/A, 10-K 405 of a company is in the the following link:\n",
    "\n",
    "##### 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&company=' + Company+Name (a space between the two companies is replaced by a '+') + '&type=10-K&dateb=&owner=exclude&count=100'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, your task will be to collect information on Company Name, Date of Filing, Filing Type and Link to the 'Document' of the filing type for each of the companies in the final dropped list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: Make sure to collect all the filing links in every companies page. Number of filings available for each company varies, so make sure you take care of that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After populating the lists, you need to save the data in the following csv file, using what you have learned from the pandas lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "from bs4 import SoupStrainer\n",
    "columns=[]\n",
    "company_name=[]   \n",
    "date_of_filing=[]   \n",
    "filing_type=[]\n",
    "link=[]\n",
    "finallink=[]\n",
    "\n",
    "for link_number in range(0,len(finalurl)):\n",
    "   \n",
    "    html = urllib.request.urlopen(finalurl[link_number]).read().decode('utf-8')\n",
    "   \n",
    "        \n",
    "    \n",
    "    soup = bs.BeautifulSoup(html)\n",
    "    \n",
    "    infotable = soup.find_all(\"table\", summary = \"Results\")\n",
    "    \n",
    "    \n",
    "    for row in infotable[0].find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        company_name.append(list_name[link_number])\n",
    "        date_of_filing.append(columns[3].getText())\n",
    "        filing_type.append(columns[0].getText())\n",
    "        link.append(columns[1])\n",
    "    #print(len(link))\n",
    "        \n",
    "    for i in range(0,len(link)):\n",
    "        start=str(link[i]).find('href')\n",
    "        end=str(link[i]).find('id=')\n",
    "        finallink=str(link[i])[start+6:end-2]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output=pd.DataFrame({\"company_name\":company_name,\n",
    "                      \"date\":date_of_filing,\n",
    "                      \"fill_type\":filing_type,\n",
    "                      \"link\":finallink\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv('10K.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
